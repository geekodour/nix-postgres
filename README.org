* nix-postgres-docker
NOTE: wip, i am still writing this out. brb doing side quests

#+begin_quote
While I think I should use a managed pg or one of those baked distributions already, I also think selfhosting pg for my homelab has taught me many lessons that I wouldn't learn otherwise, so I guess we shall continue doing so "for now". (that too in a container)

Deploying postgres on containers is [[https://www.reddit.com/r/PostgreSQL/comments/11nwf54/container_or_not/][definitely a matter]] of [[https://www.reddit.com/r/PostgreSQL/comments/1c2rbow/why_not_run_production_postgres_in_docker/][discussion]]. one of the issues i faced was docker messing up with [[https://www.instaclustr.com/blog/postgresql-docker-and-shared-memory/][dynamic shared memory configuration in postgres]], but otherwise i've been happy running postgres in a container. (i personally have been using [[https://github.com/hashicorp/nomad][nomad]] so far)

How to deploy, how to add a connection pooler etc. are concerns which are out of the scope of this repo at the moment. I'll keep this document focused on "how to setup WAL archival using WAL-G" when running postgres in a container.
#+end_quote
** Table of contents :TOC_5:
- [[#nix-postgres-docker][nix-postgres-docker]]
  - [[#context-and-the-problem][Context and the problem]]
  - [[#solution][Solution]]
    - [[#creating-the-docker-image--breakdown-of-postgres-docker-image][Creating the docker image / Breakdown of postgres docker image.]]
    - [[#setting-up-wal-g--references][Setting up ~wal-g~ & references]]
      - [[#command-reference][Command reference]]
      - [[#making-base-backup][Making base backup]]
      - [[#running-wal-archival][Running WAL archival]]
      - [[#restoration][Restoration]]
        - [[#step1-restoring-base-backup][Step1: Restoring base backup]]
        - [[#step2-restoring-the-wal][Step2: Restoring the WAL]]
      - [[#cleanup-and-maintenance][Cleanup and Maintenance]]
  - [[#other-notesreferences][Other notes/references]]
    - [[#aside-on-backup-ecosystem-in-postgres][Aside on Backup ecosystem in Postgres]]
      - [[#replication-vs-backup][Replication vs Backup]]
    - [[#everything-works-except-archive_command][Everything works except ~archive_command~!]]
    - [[#previous-attempts][Previous attempts]]
      - [[#creating-normal-dockerfile-official-docker-image][Creating normal Dockerfile (official docker image)]]
      - [[#using-dockertoolsfromimage-official-docker-image][Using ~dockerTools.fromImage~ (official docker image)]]

** Context and the problem
To give you the context, I've been happily running postgres in my homelab without a backup(who needs one anyway) but then I planned on doing a user facing side project for which I decided, will use the same pg cluster. But now, even though I'll get only 2 users it's my duty to ensure that their data remains intact despite of the unfathomable amount of fuckery i am about to unleash on my homelab servers. So, in conclusion, i need a backup policy for the database.

I already have one actually, ~pg_dump~ > [[https://restic.net/][restic]] > [[https://en.wikipedia.org/wiki/Backblaze][blackblaze B2]], this backs up more than just a db though. But, it is not ideal(various reasons) for a database that'll have a continuous flow of writes to it. For this, you need what postgres folks call "[[https://www.postgresql.org/docs/current/continuous-archiving.html][continuous archival]]" (many ways to do it, see aside section below). For sqlite, there's [[https://litestream.io/][litestream]] which does a similar thing pretty brilliantly.

So after doing some comparison of various backup solutions/methods, i decided to do with [[https://github.com/wal-g/wal-g][wal-g]]. You could actually do it without any 3rd party tool but they definitely help. (See below for comparison)

Previously I've been running a custom docker image based on the official postgres docker image to install the required extension. It'd be nice to extend it! In case of ~wal-g~, the running postgres service([[https://www.interdb.jp/pg/pgsql09/10.html][archiver (background) process]]) needs to call ~wal-g~ from the [[https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-ARCHIVE-COMMAND][archive_command]] postgres configuration. (This gets triggered every-time there's a new ~WAL segment~, which can in-turn be triggered by setting ~archive_timeout~ in case your db doesn't make enough commits to generate a new ~WAL segment~ but you still want continuous archival). Basically, you need ~wal-g~ and ~postgres~ in the same runtime and be callable(~$PATH~).

Now to get ~wal-g~ running I just need to install ~wal-g~ into the docker image(that I have already), that's a ~apt-get~. Simple right? Yes! There are even some [[https://github.com/wal-g/wal-g/issues/473][off the shelf]] [[https://github.com/stephane-klein/playground-postgresql-walg/blob/60c483c7675899bdf3a4ad3f0d7627f3998432b8/docker-image/postgres-with-wal-g/Dockerfile#L20][community]] [[https://hub.docker.com/r/apkawa/wal-g][repos]] online for this exact usecase.

But I am stupid, so I decide to make my life harder. I have rest of my infra wired nicely in [[https://zero-to-nix.com/concepts/flakes][nix flakes]] and I run things on an arm machine and also locally(x86), so I thought to myself it'd be really nice to have "most" things in a flake. Honestly would've gone the first way if I knew doing it the nix way would put me in the trenches.

So now finally our problem becomes:
#+begin_quote
"Create a multi-arch production-ready(walg+compatibility w official image+custom extensions) postgres docker image using NixOS dockerTools AND make sure the auxiliary processes(eg. base backup shipping, cleanup etc.) are running as expected"
#+end_quote
** Solution
*** Creating the docker image / Breakdown of postgres docker image.
I initially attempted to base off the official postgres docker image, this was not successful. So the alternative is building the image directly from Nix definitions. [[https://xeiaso.net/talks/2024/nix-docker-build/][This is usually pretty simple]], but for postgres things are a bit complicated you need the postgres user, postgres has initdb scripts, the official image makes use of [[https://github.com/tianon/gosu][gosu]] etc which I don't properly understand.

But it was doable, I did it in two different ways:
1. Based on official postgres docker image: Most things work nicely, but I am not sure how would I install postgres extensions to this. It will be possible just not super straightforward.
2. Based on nixpkgs postgres: Installing ~wal-g~, installing other things along with postgres extension to the docker container is pretty straightforward.

See ~./flake.nix~

Additionally, github actions is setup at ~.github/workflows~ to build this image for both ~amd64~ and ~arm64~
*** Setting up ~wal-g~ & references
#+begin_quote
Gitlab has an amazing runbook for wal-g
- https://gitlab.com/gitlab-com/runbooks/-/blob/master/docs/patroni/postgresql-backups-wale-walg.md
- https://archive.ph/83BLw

My impression on ~wal-g~, while I think it's great software but it lacks heavily in documentation. I am not sure why such a widely adopted tool has such poor documentation(lot of undocumented cli features). Lot of conceptual things which are important to understand for something like managing backup are not mentioned at all. Fortunately, they've a super friendly and nice telegram and the developers hang around and help resolve issues. I'd personally like to work on the docs but I'll let things sink in for a while.
#+end_quote
**** Command reference
Following are ~wal-g~ commands, what they do and when to run them.
| context     | wal-g command | what it does                                                                | when to run?                                                                                                                                         |
|-------------+---------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------|
| base-backup | ~backup-list~   | List of successful "base backups", also will have delta backups             | manually, to check                                                                                                                                   |
|             |               | ~backup-list --detail --pretty~ is useful as-well                             |                                                                                                                                                      |
|             | ~backup-push~   | create a full ~$PGDATA~ backup                                                | periodic(auto), via ~systemd-timer~                                                                                                                    |
|             |               | ~backup-push~ will decide on it's own when to do delta or full                |                                                                                                                                                      |
|             | ~delete~        | removes unnecessary WAL(s), leaving only stuff that can be used for restore | periodic(auto), via ~systemd-timer~                                                                                                                    |
|             | ~backup-fetch~  | Restore PostgreSQL data directory from a full backup                        | manually, when shit hits the fan                                                                                                                     |
|-------------+---------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------|
| wal         | ~wal-push~      | pushes WALs as part of ~archive_command~                                      | never run manually                                                                                                                                   |
|             | ~wal-fetch~     | fetches WALs as part of ~restore_command~                                     | never run manually, but only run during restoration. The gitlab runbook mentions that they nolonger use this, but use streaming replication directly |
|-------------+---------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------|
| replication | ~wal-receive~   | I haven't explored this yet, but can be an alternative to ~wal-push~          |                                                                                                                                                      |
|             | ~catchup-*~     | I haven't explored these yet                                                |                                                                                                                                                      |
|-------------+---------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------|
| verify      |               | There are verify and checksum check related commands aswell                 |                                                                                                                                                      |
**** Making base backup
This could be done as a [[https://developer.hashicorp.com/nomad/tutorials/task-deps/task-dependencies-interjob][sidecar]] job also in cloud native environments but since i have the machine to myself I like to setup systemd-timer. Other than the timer, I can now ssh into the machine and manually trigger the backup with ~sudo systemctl start walg-backup-push~. The systemd service looks something like:
#+begin_src nix
# walg-backup-push will do a full backup and clean unnessary wal files from the backup
systemd.services."walg-backup-push" = {
  script = ''
  set -eu
  export AWS_ENDPOINT=
  export WALG_S3_PREFIX=s3://<bucket_name>/<custom_suffix>
  export AWS_ACCESS_KEY_ID=<agenix+cat>
  export AWS_SECRET_ACCESS_KEY=<agenix+cat>
  export PGPASSWORD=<agenix>

    ${pkgs.wal-g}/bin/wal-g backup-push $WALG_PGDATA && ${pkgs.wal-g}/bin/wal-g delete garbage
  '';
  serviceConfig = {
    Type = "oneshot";
    User = "root";
  };
  environment = {
    WALG_PGDATA = "/var/lib/postgres/data";
    PGHOST = "localhost";
    PGPORT = "5432";
    PGUSER = "postgres";
  };
};
systemd.timers."walg-backup-push" = {
  wantedBy = [ "timers.target" ];
  timerConfig = {
    OnCalendar = "weekly"; # my db updates non-frequently, OK with 1 week worth of WAL
    Persistent = true;
    Unit = "walg-backup-push.service";
  };
};
#+end_src
**** Running WAL archival
- Just set ~archive_command = 'wal-g wal-push %p'~ and we're all set. It needs the env vars: ~AWS_ACCESS_KEY_ID~, ~AWS_SECRET_ACCESS_KEY~, ~AWS_ENDPOINT~, ~WALG_S3_PREFIX~ if you're using a s3 backend(blackblaze). You can set them however you want(eg. I set them using nomad env vars)
- If WAL shipping (~archive_command~) fails for some reason, WAL files will be kept on the server until the disk is running full!
**** TODO Restoration
The [[https://www.postgresql.org/docs/current/runtime-config-wal.html][official pg docs]] mention 3 different types of recovery:
- crash recovery / targeted recovery
- archive-based replication / standby mode
- streaming replication

This repo is more about crash recovery and recovering a pg cluster which was backed up using WAL archive-based backup(eg. using ~wal-g~). Some people use streaming replication as a "restore mechanism" aswell (eg. see gitlab runbook). But streaming replication as a "backup mechanism" doesn't make sense. (Eg. table gets dropped, table gets dropped in both of the primary and secondary! in this case you need WAL archival and PITR)
***** Step1: Restoring base backup
- This is more of a one time manual thing
- Find a place where you want to restore the base backup to, this will usually be the ~$PGDATA~ of the new postgres server to be restored.
- ~wal-g backup-fetch /new/path/to/restored-cluster LATEST~ (~LATEST~ here is the name of backup, the literal ~LATEST~ will just fetch the actual latest base backup done so far)
***** Step2: Restoring the WAL
- The settings(~postgresql.conf~) here are only needed for the duration of restore, once done they should be reset.
- Based on the scenario, you can do both "Standby mode" and "Targeted recovery" with the same instance, by having both ~standby.signal~ and ~recovery.signal~
****** Config reference
The official docs(pg16.04) are beter reference but just dumping here for an overview.
| Context                      | Name                      | Description                                                                       | Crash recovery mode              | Standby mode |
|------------------------------+---------------------------+-----------------------------------------------------------------------------------+----------------------------------+--------------|
|                              | ~restore_command~           |                                                                                   | Required                         | Optional     |
|------------------------------+---------------------------+-----------------------------------------------------------------------------------+----------------------------------+--------------|
| Recovery Target("till this") | ~recovery_target~           | Only allowed value is ~immediate~, till consistent state                            | Yes(oneof)                       | N/A          |
|                              | ~recovery_target_name~      | if ~pg_create_restore_point~ was used                                               | Yes(oneof)                       | N/A          |
|                              | ~recovery_target_time~      | numeric offset from UTC, time stamp up to which recovery will proceed.            | Yes(oneof)                       | N/A          |
|                              | ~recovery_target_lsn~       | If you know the exact lsn                                                         | Yes(oneof)                       | N/A          |
|                              | ~recovery_target_xid~       | If you know the exact tx_id                                                       | Yes(oneof)                       | N/A          |
|------------------------------+---------------------------+-----------------------------------------------------------------------------------+----------------------------------+--------------|
|                              | ~recovery_target_inclusive~ | Related to ~recovery_target_[time/lsn/xid]~ for one-off adjustments                 | Yes(default:on)                  | N/A          |
|                              | ~recovery_target_timeline~  | The ~recovery_target_*~ can only belong to one timeline_id.                         | Yes(default:latest wal timeline) | N/A          |
|                              | ~recovery_target_action~    | What happens when restore completes (~pause/shutdown/promote~). see doc for details | Yes(default:pause)               |              |
****** Standby mode
#+begin_quote
- Standby mode can work via streaming replication or via WAL Archival or both
- Standby mode is usually for HA and readonly, can be promoted(see hot/warm standby etc.).
#+end_quote
- create a file called ~standby.signal~ in ~$PGDATA~.
- Start the postgres server in standby mode (by virtue of ~standby.signal~)
  - The server will enter standby and will not stop being standby utill promoted (if ~hotstandby~, we'll be able to read from it aswell)
- Based on your configuration, Fetches new data based on either or both:
  - ~primary_conninfo~ : Sending server (streaming replication). When using this, ~recovery_*~ configuration is mostly not useful.
  - ~restore_command~: fetching new WAL segments.
****** Targeted recovery/Crash recovery/Data loss recovery mode
#+begin_quote
PITR, as I understand PITR, it's is not one single thing. It's the combination of all the different recovery targets, mechanism to do the backups and the concept of [[https://www.highgo.ca/2021/11/01/the-postgresql-timeline-concept/][postgres timeline]] etc. With the concept of timeline ID, it is possible that the same LSN or the same WAL segments exist in multiple timelines.
#+end_quote
- create a file called ~recovery.signal~ in ~$PGDATA~.
  - The server will enter recovery and will not stop recovery when the end of archived WAL is reached
  - The ~recovery_*~ configuration control how this behaves (Eg. you can control for an earlier stopping point than end of WAL etc.)
- Set the required ~recovery_*~ settings based on requirements
  - Basic restore config for ~postgresql.conf~
    - ~restore_command = wal-g wal-fetch %f %p~
      - It needs the env vars: ~AWS_ACCESS_KEY_ID~, ~AWS_SECRET_ACCESS_KEY~, ~AWS_ENDPOINT~, ~WALG_S3_PREFIX~ if you're using a s3 backend(blackblaze)
    - ~recovery_target = 'immediate'~
- Start the postgres server in recovery mode (by virtue of ~recovery.signal~)
- This should do the recovery till you reach ~recovery_target_action~.
- If everything looks good, remove the ~recovery.signal~ file from ~$PGDATA~.
**** Cleanup and Maintenance
- If you make a ~backup-push~ with the ~--parmanent~ flag(or using ~backup-mark~), it'll not be picked by the ~delete~ command.
- ~wal-g delete garbage~ can be run periodically to remove WAL files which
- I as of the moment run it whenever I make a base backup, but this may differ on your backup policy for your infra
** Other notes/references
*** TODO Aside on Backup ecosystem in Postgres
#+begin_quote
This section is todo, I'll update later.
#+end_quote
Now I observe the postgres community from the sidelines and I don't think I have any "real" postgres experience, however this is what i've figured reading the manual and opinion of other folks on the interwebs.
#+begin_quote
"pgBackRest is more conservative and kind of reliable, wal-g is more performant, both are much better than Barman"

I won't mention who said it, my real surname is ~Barman~ (check my github fr) and I am not offended.
#+end_quote
**** Replication vs Backup
*** Everything works except ~archive_command~!
#+begin_quote
This is a short debugging story, that finally had a happy ending. :)
#+end_quote
Creating the plain postgres container image from nixpkgs using ~dockerTools~ was simple. It builds off ~24.05~ which has a postgres version pinned, and it happens to be ~postgres16.4.x~ and built using ~glibc 2.39~ but I was already running postgres already using the official debian based docker image. When I deployed the custom image to replace the official image, I got collation version mismatch [[https://community.fly.io/t/postgres-flex-database-postgres-has-a-collation-version-mismatch/14391][similar to this issue]]. I initially thought this was related to ~locale~, but later confirmed it was related the glibc version the postgres binary was compiled with and run from. The usually suggested way out of this is to ~REINDEX~ but my tables don't even have a backup yet so I didn't want to take any risk whatsoever.

#+begin_src
WARNING:  database "<name>" has a collation version mismatch
DETAIL:  The database was created using collation version 2.36, but the operating system provides version 2.39.
HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE <name> REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.
#+end_src

So for this, there were too many ways to fix this. I want to keep things simple, so I just bought back a previous failed attempt, instead of building from nixpkgs, I'll use ~fromImage~ and use the official postgres image as base and install nixpkgs ~wal-g~ on top of it.

This initially failed because of [[https://github.com/NixOS/nix/issues/1205#issuecomment-2161613130][this issue]] (the ~docker_entrypoint.sh~ in the official pg image had ~/usr/bin/env~) but we resolved it using ~dockerTools.buildLayeredImage.fakeRootCommands~. So now everything is working!

Old postgres official image is replaced by custom docker image, there's no collation mismatch, I am able to exec into the container where postgres is running and am able to access the ~wal-g~ binary in the ~$PATH~. All G!

Now things were straightforward, I just had to setup the ~archive_command~ to ~wal-g wal-push %p~ and I am good. So I did that.

I got hit by dreaded boi 127:
#+begin_src shell
archive command failed with exit code 127
#+end_src

This drove me nuts. Initially I thought the forked ~archiver~ process was not able to access ~wal-g~ somehow. So I tried doing normal ~cp~ as the ~archive_command~ as that's the simplest. That was failing too!

Then I tried:
- ~archive_command = 'echo test > /tmp/archive_test.txt'~ : fail
- ~archive_command = 'true'~ : fail!

Now since ~/bin/true~ was failing I realized, I might have fu*ked up big time. To confirm that this is in-fact my image, I tried setting the ~archive_command = 'true'~ in the official postgres image and it was working. So I was sure that whatever is happening is my doing.

After some googling and claude, I found that you could trigger archive on demand by manually doing a WAL switch using: ~SELECT pg_switch_wal();~ So now could observe the postgres process at syscall level when it happens and not get lost in the sea of syscall. Let's bring in old friend ~strace~.

- ~docker inspect -f '{{.State.Pid}}' <container id>~ : get the pid of the running postgres container (main process)
- ~sudo strace -f -p <pid> -s 1024 -o /tmp/postgres_strace.log~
- Then manually trigger the WAL switch and hence ~archival_command~ and exit out of strace.
- Inspect the log, I see:
  #+begin_src
1053745 execve("/bin/sh", ["sh", "-c", "true"], 0xaaaaecdfef10 /* 65 vars */ <unfinished ...>
1053746 close(3)                        = 0
1053745 <... execve resumed>)           = -1 ENOENT (No such file or directory)
1053746 signalfd4(-1, [URG], 8, SFD_CLOEXEC|SFD_NONBLOCK <unfinished ...>
1053745 exit_group(127)                 = ?
  #+end_src
- It's not able to find ~true~! Upon looking up,  ~/bin/true~ exists. Hmm.
- Look closer.
- It's not able to find ~sh~! Upon looking up,  ~/bin/sh~ does not exists :)
- Now this is the same nix issue as not finding ~env~ as previously mentioned.

So I added the fix in similar manner using: ~ln -sfn "${pkgs.bash}/bin/sh" /bin/sh~ and things finally started working.
*** Previous attempts
**** Creating normal Dockerfile (official docker image)
This image has a extension that i use but doesn't have wal-g in it.
#+begin_src Dockerfile
FROM postgres:16.2-bookworm AS builder

RUN apt-get update \
    && apt-get -y upgrade \
    && apt-get install -y --no-install-recommends \
    curl ca-certificates git\
    build-essential libpq-dev postgresql-server-dev-all
RUN update-ca-certificates

WORKDIR /srv
RUN git clone https://github.com/fboulnois/pg_uuidv7.git .
RUN for v in `seq 16`; do pg_buildext build-$v $v; done

# create tarball and checksums
RUN cp sql/pg_uuidv7--1.5.sql . && TARGETS=$(find * -name pg_uuidv7.so) \
  && tar -czvf pg_uuidv7.tar.gz $TARGETS pg_uuidv7--1.5.sql pg_uuidv7.control \
  && sha256sum pg_uuidv7.tar.gz $TARGETS pg_uuidv7--1.5.sql pg_uuidv7.control > SHA256SUMS

FROM postgres:16.2-bookworm AS runner

COPY --from=builder /srv/pg_uuidv7.tar.gz /srv/SHA256SUMS /srv/
COPY --from=builder /srv/${PG_MAJOR}/pg_uuidv7.so /usr/lib/postgresql/${PG_MAJOR}/lib
COPY --from=builder /srv/pg_uuidv7.control /usr/share/postgresql/${PG_MAJOR}/extension
COPY --from=builder /srv/pg_uuidv7--1.5.sql /usr/share/postgresql/${PG_MAJOR}/extension
#+end_src
**** Using ~dockerTools.fromImage~ (official docker image)
- ~dockerTools~ allow you pull from another image. So since the official comes with the starter scripts that run initdb and it does a lot of other things. I thought it would be nice to just use it. But it didn't work out.
- See [[https://discourse.nixos.org/t/building-on-dockerfile-based-images/29583][Building on dockerfile-based images - Help - NixOS Discourse]]
#+begin_src nix
# file: flake.nix
let
  # nix run nixpkgs#nix-prefetch-docker -- postgres --image-tag 16.2-bookworm --arch amd64 --os linux
  pg_amd64 = pkgs.dockerTools.pullImage {
    imageName = "postgres";
    imageDigest = "sha256:4aea012537edfad80f98d870a36e6b90b4c09b27be7f4b4759d72db863baeebb";
    sha256 = "1rizfs2f6l834cgym0jpp88g3r3mcrxn9fd58np91ny9fy29zyck";
    finalImageName = "postgres";
    finalImageTag = "16.2-bookworm";
    os = "linux";
    arch = "amd64";
  };
in {
  packages = {
    nix_postgres_docker = pkgs.dockerTools.buildLayeredImage  {
      name = builtins.getEnv "IMAGE_NAME";
      tag = builtins.getEnv "IMAGE_TAG";
      fromImage = pg_amd64; # TODO make conditional
      contents = with pkgs; [ cacert postgresql16Packages.pg_uuidv7 ];
      config = {
        Cmd = ["postgres"];
        entrypoint = [ "docker-entrypoint.sh" ];
      };
    };
  };
};
#+end_src

While this seems like it works from the face of it. Unfortunately or fortunately, it doesn't. @NoobZ and @ManoftheSea from the unofficial NixOS discord channel talked me out of this and I stopped trying to make this work. If someone is interested to make this work, here is where I got stuck: "no such file or directory". I am not sure why this happened, arch is what it should be and base image is a docker image so should not be nixos fsh issues.

#+begin_quote
Update!
I think i've found the issue, it's related to the shebang on top of the entrypoint file but eitheway I have dropped the idea of using ~fromImage~ for now and going all in on building it from nix derivations.

[[https://github.com/NixOS/nix/issues/1205#issuecomment-2161613130][Inconsistent treatment of /usr/bin/env in build sandbox vs. NixOS · Issue #1205 · NixOS/nix · GitHub]]

Update 2!
Because I was having difficulty building pg16.2 from nixpkgs, I ended up using this approach as I had an exsiting db using pg16.2 and I faced a [[https://community.fly.io/t/postgres-flex-database-postgres-has-a-collation-version-mismatch/14391][collation]] issue due to glibc version and I didn't want to REINDEX yet.
#+end_quote

Trying to install nixpkgs pg extensions into the correct location in the image would've been a real hassle.

#+begin_src bash
λ just docker-build # success
λ just docker-load # success
λ just docker-run # fail
exec /usr/local/bin/docker-entrypoint.sh: no such file or directory
error: Recipe `docker-run` failed on line 36 with exit code 1
#+end_src
:(
